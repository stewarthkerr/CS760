% LaTeX generated by formulasheet.com %
\documentclass[10pt,fleqn]{minimal} 						
\usepackage{amssymb,amsmath,bm,color,siunitx} 
\usepackage[utf8]{inputenc} 
\usepackage{mathtools}
\setlength{\parindent}{0in}
\setlength{\topmargin}{-0.5in} 
\setlength{\textheight}{10in}\setlength{\evensidemargin}{0in}
\setlength{\oddsidemargin}{0in}\setlength{\textwidth}{5in} 
\setlength{\mathindent}{0in}
\definecolor{textcolour}{RGB}{0,0,0}
\mathtoolsset{showonlyrefs}

\begin{document} \pagestyle{empty}
{\normalsize { \color{textcolour}

\textbf{\underline{CS760}} \hspace{20pt}(created using formulasheet.com) \\ \\ 
\begin{equation}
	P(a_i|s) = \frac{c^{\hat{Q}(s,a_i)}}{\sum_j c^{\hat{Q}(s,a_j)}}
\end{equation}
\begin{equation}
	\pi^8 (s) = \arg \max_a Q(s,a)
\end{equation}
\begin{equation}
	Q(s,a) = E[r(s,a)] + \gamma E_{s'|s,a} [V^*(s')]
\end{equation}
\begin{equation}
	\pi^* = \arg \max_\pi V^\pi (s) \quad \forall s
\end{equation}
\begin{equation}
	V^{\pi} (s) = \sum_{t=0}^\infty \gamma^t E[r_t]
\end{equation}
\begin{equation}
	E_D \left[ \left( f(x;D) - E[y|x]\right)^2 \right] = \left( E_d[f(x;D)] - E[y|x]\right)^2 + E_D \left[ (f(x;D) - E_D[f(x;D)])^2\right]
\end{equation}
\begin{equation}
	E \left[  \left(y-f(x; D)\right)^2 \right] = E \left[  \left(y-E[y|x]\right)^2 \mid x,D \right] + \left( f(x;D) - E[y|x]\right)^2
\end{equation}
\begin{equation}
	m < \max \left( \frac{1}{\varepsilon} \log \left(\frac{1}{\delta}\right), \frac{\text{VC-dim}(C) -1}{32\varepsilon}\right) \implies \text{w.p. } \delta, \; error_D(h) > \varepsilon
\end{equation}
\begin{equation}
	m \geq \frac{1}{\varepsilon^2} \left( \ln |H| + \ln \left(\frac{1}{\delta} \right) \right)
\end{equation}
\begin{equation}
	error_{D}(h) = P_{x \in D} \left( c(x) \neq h(x)\right)
\end{equation}
\begin{equation}
	error(h) \leq error_D(h) + \sqrt{\frac{VC (\log(2m/VC) + 1) + \log(4/\delta)}{m}}
\end{equation}
\begin{equation}
	VC \leq \frac{4R^2}{\text{margin}_D(h)^2}
\end{equation}
\begin{equation}
	k(x,z) = \exp \left( -\gamma ||x-z||^2 \right)
\end{equation}
\begin{equation}
	k(x,z) = (x \cdot z + 1)^d
\end{equation}
\begin{equation}
	k(x,z) = (x \cdot z)^d
\end{equation}
\begin{equation}
	\max_{\alpha_1,\dots, \alpha_m} \sum_{i=1}^m \alpha_i - \frac{1}{2} \sum_{j=1}^m \sum_{i=1}^m \alpha_j \alpha_k y^{(j)} y^{(k)} \left(\bold{x}^{(j)} \cdot \bold{x}^{(k)} \right) \quad \text{ s.t. } \alpha_i \geq 0, \sum_{i=1}^m \alpha_i y^{(i)} = 0
\end{equation}
\begin{equation}
	h(x) = \begin{cases} +1 \quad \text{if } \bold{w}^T \bold{x} + b > 0 \\ -1 \quad \text{o/w} \end{cases}
\end{equation}
\begin{equation}
	\min_{\bold{w},b,\xi^{(i)}} \frac{1}{2} || \bold{w}||_2^2 + C \sum_{i=1}^m \xi^{(i)}\quad \text{ s.t. } y^{(i)} (\bold{w}^T \bold{x}^{(i)} + b) \geq 1-\xi^{(i)}, \; \xi^{(i)} \geq 0 \; \forall i
\end{equation}
\begin{equation}
	\min_{\bold{w},b} \frac{1}{2} || \bold{w}||_2^2 \quad \text{ s.t. } y^{(i)} (\bold{w}^T \bold{x}^{(i)} + b) \geq 1 \; \forall i
\end{equation}
\begin{equation}
	\text{margin}_D (h) = \frac{1}{2} \bold{\hat{w}}^T (x_+ - x_-) = \frac{1}{||\bold{w}||_2}
\end{equation}
\begin{align}
	E(\bold{\beta}) = || \bold{y} - X \bold{\beta} ||_2^2  + \lambda ||\bold{\beta}||_1
\end{align}
\begin{align}
	E(\bold{\beta}) = || \bold{y} - X \bold{\beta} ||_2^2  + \lambda ||\bold{\beta}||_2^2
\end{align}
\begin{align}
	\hat{\beta} = X^TX
\end{align}
\begin{align}
	\min_G \max_D E_{x \sim p_{data}} \log D(\bold{x}) + E_{z} \log \left(1 - D(G(\bold{z})) \right) \\
\end{align}
\begin{align}
	E \left(\theta^{(D)} \right) &= - \frac{1}{2} E_{x \sim p_{data}} \log D(\bold{x}) - \frac{1}{2} E_{z} \log \left(1 - D(G(\bold{z})) \right) \\
	E \left(\theta^{(G)} \right) &= - E \left(\theta^{(D)} \right)
\end{align}
\begin{equation}
	f(net) = \frac{e^{net_j}} {\sum_j e^{net_j}}
\end{equation}
\begin{equation}
	\delta_j = - \frac{\partial E}{\partial net_j} = \text{e.g.} \; o_j (1-o_j) \sum_k \delta_k w_{kj}
\end{equation}
\begin{equation}
	\Delta w_{ji} = - \eta \delta_j o_i
\end{equation}
\begin{equation}
	\Delta \bold{w} = - \eta \nabla_\bold{w} E(\bold{w})
\end{equation}
\begin{equation}
	f(net) = \begin{cases} 0 \quad \text{if } x < 0 \\ x \quad \text{if } x \geq 0 \end{cases}
\end{equation}
\begin{equation}
	f(net) = \tanh(x) = \frac{2}{1+e^{-2net}} - 1
\end{equation}
\begin{equation}
	f(net) = \frac{1}{1 + e^{-net}}
\end{equation}
\begin{equation}
	net = w_0 + \sum_i w_i x_i
\end{equation}
\begin{equation}
	E(\bold{w}) = -\sum_{d \in D} \sum_{i=1}^{\#class} y_i^{(d)} \ln \left( o_i^{(d)}\right)
\end{equation}
\begin{equation}
	E(\bold{w}) = \sum_{d \in D} - y^{(d)} \ln \left( o^{(d)}\right) - \left(1 - y^{(d)}\right) \ln \left( 1-o^{(d)} \right)
\end{equation}
\begin{equation}
	E(\bold{w}) = \frac{1}{2} \sum_{d \in D} \left(y^{(d)} - o^{(d)} \right)^2
\end{equation}
\begin{equation}
	m \geq \frac{1}{\varepsilon} \left( \ln |H| + \ln \left(\frac{1}{\delta} \right) \right)
\end{equation}
\begin{equation}
	m \geq \frac{1}{\varepsilon} \left( 4 \log_2 \left(\frac{2}{\delta} \right) + 8 \text{VC-dim}(H) \log_2 \left(\frac{13}{\varepsilon} \right) \right)
\end{equation}
\begin{equation}
	Z = \sum_{\bold{v} \in \bold{V}} \prod_k \phi_k(\bold{v})
\end{equation}
\begin{equation}
	P(\bold{V}) = \frac{1}{Z} \prod_k \phi_k(\bold{V})
\end{equation}
\begin{equation}
	P(Y=y|\bold{x}) = \frac{P(y)P(\bold{x}|y)}{\sum_{y'}P(y')P(\bold{x}|y')}
\end{equation}
\begin{equation}
	I(X_i,X_j|Y) = \sum_{x_i} \sum_{x_j} \sum_{y}  P(x_i,x_y,y) \log_2 \frac{P(x_i,x_j|y)}{P(x_i|y)P(x_j|y)}
\end{equation}
\begin{equation}
	P(Y=y|X) = \frac{P(y)\prod_{i=1}^n P(x_i|y)}{\sum_{y' \in \text{values}(Y)} P(y')\prod_{i=1}^n P(x_i|y')}
\end{equation}
\begin{equation}
	P(X_1,\dots,X_n,Y) = P(Y) \prod_{i=1}^n P(X_i|Y)
\end{equation}
\begin{equation}
	I(X,Y) = \sum{x \in \text{values}(X)} \sum{y \in \text{values}(Y)} P(x,y) \log_2 \frac{P(x,y)}{P(x)P(y)}
\end{equation}
\begin{equation}
	\text{M-step: } P(a|b,e) = \frac{E\#(a \wedge b \wedge e)}{E\#( b \wedge e)}
\end{equation}
\begin{equation}
	\text{m-estimates: } P(X=x) = \frac{n_x+p_xm}{\left(\sum_{v \in \text{values}(X)} n_v \right) + m}
\end{equation}
\begin{equation}
	\text{MAP (Laplace): } P(X=x) = \frac{n_x+1}{\sum_{v \in \text{values}(X)} (n_v+1)}
\end{equation}
\begin{equation}
	P(b|j,m) = \frac{P(b,j,m)}{P(j,m)} = \frac{P(b,j,m)}{P(b,j,m) + P(\neg b,j,m)}
\end{equation}
\begin{equation}
	P(X_1, \dots,X_n) = \prod_{i=1}^n P(X_i | \text{Parents}(X_i))
\end{equation}
\begin{equation}
	t = \frac{\bar{\delta}}{\sqrt{\frac{1}{n(n-1)}\sum_{i=1}^n (\delta_i - \bar{\delta})^2}}
\end{equation}
\begin{equation}
	\text{Error CI: } error_S{h} \pm z_{\alpha} \sqrt{\frac{error_S(h)(1 - error_S(h))}{n}}
\end{equation}
\begin{equation}
	\text{precision} = \frac{\text{TP}}{\text{TP}+\text{FP}}
\end{equation}
\begin{equation}
	\text{FPR} = \frac{\text{FP}}{\text{TN}+\text{FP}}
\end{equation}
\begin{equation}
	\text{TPR (recall)} = \frac{\text{TP}}{\text{TP} + \text{FN}}
\end{equation}
\begin{equation}
	\text{Bias}[\hat{\theta}] = E[\hat{\theta}] - \theta
\end{equation}
\begin{equation}
	\sum_{i=1}^{|D|} (y_i - \hat{y_i})^2 = \sum_{L \in \text{leaves}} \sum_{i\in L} (y_i - \hat{y_i})^2
\end{equation}
\begin{equation}
	h \in H \text{ overfits if } \exists h' \in H \text{ s.t. } error(h) > error(h') \text{ and } error_D(h) < error_D(h')
\end{equation}
\begin{equation}
	\text{GainRatio} (D,S) = \frac{\text{InfoGain}(D,S) }{\text{SplitInfo}(D,S) }
\end{equation}
\begin{equation}
	\text{SplitInfo}(D,S) = -\sum_{k \in \text{outcomes}(S)} \frac{|D_k|}{|D|} \log_2 \left(  \frac{D_k}{D} \right)
\end{equation}
\begin{equation}
	\text{InfoGain}(D,S) = H_D(Y) - H_D(Y|S)
\end{equation}
\begin{equation}
	H(Y|X=x) = - \sum_{y \in \text{values}(Y)} P(Y=y|X=x) \log_2 P(Y=y |X=x)
\end{equation}
\begin{equation}
	H(Y|X) = \sum_{x \in \text{values}(X)} P(X=x) H(Y|X=x)
\end{equation}
\begin{equation}
	H(Y) = - \sum_{y \in \text{values}(Y) } P(y) \log_2 P(y)
\end{equation}

}}
\end{document}